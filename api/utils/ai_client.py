import os
import json
from typing import List, Dict, Any

import httpx


API_KEY = os.getenv("UNIAPI_API_KEY")
BASE_URL = os.getenv("UNIAPI_BASE_URL", "https://hk.uniapi.io/v1").rstrip("/")


async def _create_chat_completion(
    *,
    model: str,
    messages: List[Dict[str, Any]],
    temperature: float = 0.3,
    max_tokens: int | None = None,
    response_format: Dict[str, Any] | None = None,
) -> str:
    """è°ƒç”¨ UniAPI(OpenAI å…¼å®¹) çš„ /chat/completions æ¥å£ï¼Œè¿”å› content å­—ç¬¦ä¸²ã€‚
    æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰ã€‚
    """

    if not API_KEY:
        raise RuntimeError("UNIAPI_API_KEY is not set in environment variables")

    url = f"{BASE_URL}/chat/completions"

    # å¤„ç†æ¶ˆæ¯ï¼Œæ”¯æŒå›¾ç‰‡
    processed_messages = []
    for msg in messages:
        if "images" in msg and msg["images"]:
            # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            content_parts = []
            
            # æ·»åŠ æ–‡æœ¬éƒ¨åˆ†
            if msg.get("content"):
                content_parts.append({
                    "type": "text",
                    "text": msg["content"]
                })
            
            # æ·»åŠ å›¾ç‰‡éƒ¨åˆ†
            for img in msg["images"]:
                content_parts.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{img['mime_type']};base64,{img['base64']}"
                    }
                })
            
            processed_messages.append({
                "role": msg["role"],
                "content": content_parts
            })
        else:
            # æ™®é€šæ–‡æœ¬æ¶ˆæ¯
            processed_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })

    payload: Dict[str, Any] = {
        "model": model,
        "messages": processed_messages,
        "temperature": temperature,
    }
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens
    if response_format is not None:
        payload["response_format"] = response_format

    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json",
    }

    async with httpx.AsyncClient(timeout=60.0) as client:  # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥å¤„ç†å›¾ç‰‡
        resp = await client.post(url, headers=headers, json=payload)
        resp.raise_for_status()

    data = resp.json()
    # å…¼å®¹ OpenAI é£æ ¼çš„è¿”å›ç»“æ„
    return data["choices"][0]["message"]["content"].strip()

async def get_next_question(history: List[Dict[str, Any]], model: str = "grok-4-1-fast-non-reasoning") -> Dict[str, Any]:
    """
    æ ¹æ®å¯¹è¯å†å²ï¼Œå†³å®šæ˜¯ç»§ç»­æé—®è¿˜æ˜¯åœæ­¢ã€‚
    """
    system_prompt = """ä½ æ˜¯ä¸€ä½æ¸©æš–è´´å¿ƒçš„ä¸‰ç”²åŒ»é™¢åˆ†è¯ŠåŒ»ç”ŸåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯é€šè¿‡è¯¢é—®æ‚£è€…ç—‡çŠ¶æ¥æ”¶é›†ä¿¡æ¯ï¼Œä»¥ä¾¿è¿›è¡Œåˆæ­¥åˆ†è¯Šã€‚

è¯·ç”¨å…³æ€€ã€æ¸©æš–çš„è¯­æ°”ä¸æ‚£è€…äº¤æµï¼Œå°±åƒä¸€ä½çœŸæ­£å…³å¿ƒæ‚£è€…å¥åº·çš„åŒ»ç”Ÿã€‚

æ‚£è€…æ˜¯å¯ä»¥ä¸Šä¼ ç…§ç‰‡çš„ï¼Œå¦‚æœæœ‰ï¼Œè¯·æ ¹æ®å›¾ç‰‡å†…å®¹è¿›è¡Œåˆ†æã€‚

è§„åˆ™ï¼š
1. æ¯æ¬¡åªé—®**ä¸€ä¸ª**æœ€å…³é”®çš„è¡¥å……é—®é¢˜ã€‚
2. æ ¹æ®å¯èƒ½çš„ç›¸å…³å¤šç§ç–¾ç—…ï¼Œè§£æç—‡çŠ¶ï¼Œä¸ºç—…äººæä¾›å¯é€‰æ‹©çš„ç—‡çŠ¶åˆ—è¡¨è¿›è¡Œç¡®è®¤ã€‚
3. é€‰æ‹©é¡¹åº”è¯¥ç®€æ´æ˜äº†ï¼Œ4-6ä¸ªé€‰é¡¹ä¸ºå®œï¼ŒåŒ…æ‹¬å¸¸è§ç›¸å…³ç—‡çŠ¶ã€‚
4. å¦‚æœé—®é¢˜ä¸é€‚åˆæä¾›é€‰é¡¹ï¼ˆå¦‚éœ€è¦è¯¦ç»†æè¿°ï¼‰ï¼Œå¯ä»¥åªè¿”å›é—®é¢˜æ–‡æœ¬ã€‚
5. å¦‚æœä½ è®¤ä¸ºå½“å‰æ”¶é›†çš„ä¿¡æ¯å·²ç»è¶³å¤Ÿåˆ¤æ–­å¤§æ¦‚çš„ç–¾ç—…æ–¹å‘å’ŒæŒ‚å·ç§‘å®¤ï¼Œè¯·ç›´æ¥å›å¤ "STOP_ASKING"ã€‚
6. ä¸¥ç¦åœ¨æ­¤æ—¶ç»™å‡ºè¯Šæ–­ç»“æœï¼Œåªè´Ÿè´£æé—®ã€‚
7. è¯­æ°”è¦æ±‚ï¼š
   - ç”¨æ¸©æš–å…³æ€€çš„è¯­æ°”ï¼ŒåƒåŒ»ç”Ÿå…³å¿ƒæ‚£è€…ä¸€æ ·äº¤æµ


è¾“å‡ºæ ¼å¼ï¼š
- å¦‚æœéœ€è¦ç»§ç»­æé—®ä¸”æä¾›é€‰é¡¹ï¼š{"question": "é—®é¢˜å†…å®¹", "options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3"]}
- å¦‚æœéœ€è¦ç»§ç»­æé—®ä½†ä¸æä¾›é€‰é¡¹ï¼š{"question": "é—®é¢˜å†…å®¹"}
- å¦‚æœä¿¡æ¯è¶³å¤Ÿï¼š{"status": "stop"}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•markdownæ ‡è®°ã€‚"""

    messages = [{"role": "system", "content": system_prompt}]
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ“¸ [AI Client] å¤„ç†å†å²æ¶ˆæ¯ï¼Œå…± {len(history)} æ¡")
    for i, msg in enumerate(history):
        has_images = "images" in msg and msg.get("images")
        print(f"  æ¶ˆæ¯ {i+1}: role={msg.get('role')}, æœ‰å›¾ç‰‡={has_images}, å›¾ç‰‡æ•°é‡={len(msg.get('images', []))}")
        if has_images:
            for j, img in enumerate(msg.get("images", [])):
                print(f"    å›¾ç‰‡ {j+1}: mime_type={img.get('mime_type')}, base64é•¿åº¦={len(img.get('base64', ''))}")
    
    messages.extend(history)

    try:
        content = await _create_chat_completion(
            model=model,
            messages=messages,
            temperature=0.3,
            max_tokens=1000,
            response_format={"type": "json_object"},
        )

        result = json.loads(content)
        
        if "status" in result and result["status"] == "stop":
            return {"status": "stop"}
        elif "question" in result:
            return {"status": "continue", **result}
        else:
            return {"status": "error", "message": "Invalid response format"}
            
    except Exception as e:
        print(f"Error calling AI: {e}")
        return {"status": "error", "message": str(e)}

async def generate_diagnosis(history: List[Dict[str, Any]], model: str = "grok-4-1-fast-non-reasoning") -> Dict[str, Any]:
    """
    æ ¹æ®å®Œæ•´å¯¹è¯å†å²ç”Ÿæˆåˆ†è¯ŠæŠ¥å‘Šã€‚
    """
    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç”Ÿã€‚æ ¹æ®ä»¥ä¸‹çš„æ‚£ç—…ä¸»è¯‰å’Œé—®è¯Šè®°å½•ï¼ˆåŒ…æ‹¬æ‚£è€…ä¸Šä¼ çš„å›¾ç‰‡ï¼‰ï¼Œè¯·ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„åˆ†è¯ŠæŠ¥å‘Šã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å« Markdown æ ¼å¼æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ï¼š
{
    "possible_conditions": ["ç–¾ç—…1", "ç–¾ç—…2"],
    "department": "å»ºè®®æŒ‚å·ç§‘å®¤",
    "urgency": "ç´§æ€¥ç¨‹åº¦ï¼ˆæ™®é€š/å»ºè®®å°½å¿«/æ€¥è¯Šï¼‰",
    "advice": "å…·ä½“çš„åŒ»ç–—å»ºè®®å’Œæ³¨æ„äº‹é¡¹",
    "reasoning_steps": [
        {
            "step": 1,
            "type": "symptom",
            "title": "ç—‡çŠ¶åˆ†æ",
            "content": "æ‚£è€…ä¸»è¦ç—‡çŠ¶æè¿°"
        },
        {
            "step": 2,
            "type": "analysis",
            "title": "åˆæ­¥åˆ†æ",
            "content": "åŸºäºç—‡çŠ¶çš„å¯èƒ½ç–¾ç—…åˆ†æ"
        },
        {
            "step": 3,
            "type": "conclusion",
            "title": "è¯Šæ–­ç»“è®º",
            "content": "æœ€ç»ˆåˆ¤æ–­å’Œå»ºè®®ç§‘å®¤"
        }
    ],
    "sankey_data": {
        "nodes": [
            {"id": "symptom_1", "name": "ç—‡çŠ¶åç§°", "layer": 0, "category": "ç—‡çŠ¶", "color": "#ec4899"},
            {"id": "symptom_2", "name": "ç—‡çŠ¶åç§°", "layer": 0, "category": "ç—‡çŠ¶", "color": "#ec4899"},
            {"id": "symptom_3", "name": "ç—‡çŠ¶åç§°", "layer": 0, "category": "ç—‡çŠ¶", "color": "#ec4899"},
            {"id": "analysis_1", "name": "ç—‡çŠ¶æ¨¡å¼", "layer": 1, "category": "åˆ†æ", "color": "#ec4899"},
            {"id": "analysis_2", "name": "ç—‡çŠ¶æ¨¡å¼", "layer": 1, "category": "åˆ†æ", "color": "#ec4899"},
            {"id": "analysis_3", "name": "ç—‡çŠ¶æ¨¡å¼", "layer": 1, "category": "åˆ†æ", "color": "#ec4899"},
            {"id": "condition_1", "name": "ç–¾ç—…1", "layer": 2, "category": "ç–‘ä¼¼æ‚£ç—…", "color": "#10b981"},
            {"id": "condition_2", "name": "ç–¾ç—…2", "layer": 2, "category": "ç–‘ä¼¼æ‚£ç—…", "color": "#10b981"}
        ],
        "links": [
            {"source": "symptom_1", "target": "analysis_1", "value": 0.8},
            {"source": "symptom_1", "target": "analysis_2", "value": 0.7},
            {"source": "symptom_2", "target": "analysis_2", "value": 0.9},
            {"source": "symptom_3", "target": "analysis_3", "value": 0.9},
            {"source": "analysis_1", "target": "condition_1", "value": 0.8},
            {"source": "analysis_2", "target": "condition_1", "value": 0.7},
            {"source": "analysis_2", "target": "condition_2", "value": 0.6},
            {"source": "analysis_3", "target": "condition_2", "value": 0.5}
        ]
    }
}

reasoning_stepsè¯´æ˜ï¼š
- step: æ­¥éª¤åºå·
- type: æ­¥éª¤ç±»å‹ï¼ˆsymptom/analysis/conclusionï¼‰
- title: æ­¥éª¤æ ‡é¢˜
- content: è¯¥æ­¥éª¤çš„å…·ä½“å†…å®¹
- é€šå¸¸3-5ä¸ªæ­¥éª¤å³å¯ï¼ŒæŒ‰é€»è¾‘é¡ºåºæ’åˆ—

sankey_dataè¯´æ˜ï¼š
- nodes: æ¡‘åŸºå›¾èŠ‚ç‚¹ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
  * id: å”¯ä¸€æ ‡è¯†ç¬¦
  * name: èŠ‚ç‚¹æ˜¾ç¤ºåç§°
  * layer: å±‚çº§ï¼ˆ0=ä¸»è¯‰ç—‡çŠ¶ï¼Œ1=åˆ†æå±‚ï¼Œ2=ç–‘ä¼¼æ‚£ç—…ï¼‰
  * category: èŠ‚ç‚¹åˆ†ç±»ï¼ˆç—‡çŠ¶/å‘ç—…æ—¶é—´/éƒ¨ä½/åˆ†æ/æ—¶é—´åˆ†æ/éƒ¨ä½åˆ†æ/ç–‘ä¼¼æ‚£ç—…ï¼‰
  * color: èŠ‚ç‚¹é¢œè‰²ï¼ˆç—‡çŠ¶ç”¨#ec4899ç²‰è‰²ï¼Œå‘ç—…æ—¶é—´ç”¨#3b82f6è“è‰²ï¼Œéƒ¨ä½ç”¨#f59e0bæ©™è‰²ï¼Œåˆ†æå±‚ç»§æ‰¿å¯¹åº”ç—‡çŠ¶é¢œè‰²ï¼Œç–‘ä¼¼æ‚£ç—…ç”¨#10b981ç»¿è‰²ï¼‰
- links: èŠ‚ç‚¹é—´çš„è¿æ¥ï¼Œvalueè¡¨ç¤ºå…³è”å¼ºåº¦ï¼ˆ0-1ä¹‹é—´ï¼‰
- ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹çš„idåœ¨nodesä¸­å”¯ä¸€ï¼Œlinksä¸­çš„sourceå’Œtargetå¼•ç”¨nodesä¸­çš„id
- ç—‡çŠ¶åº”è¯¥æŒ‰ç±»å‹åˆ†ç±»ï¼šå…·ä½“ç—‡çŠ¶æè¿°ã€å‘ç—…æ—¶é—´ã€éƒ¨ä½ç­‰
- åˆ†æå±‚åº”è¯¥å¯¹ç—‡çŠ¶è¿›è¡Œæç‚¼å’Œå½’çº³ï¼Œå½¢æˆåŒ»å­¦æœ¯è¯­
- ä¸€ä¸ªç—‡çŠ¶å¯ä»¥è¿æ¥åˆ°å¤šä¸ªåˆ†æç»“æœï¼Œä¸€ä¸ªåˆ†æç»“æœå¯ä»¥è¿æ¥å¤šä¸ªç–‘ä¼¼ç–¾ç—…
"""

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(history)

    try:
        content = await _create_chat_completion(
            model=model,
            messages=messages,
            temperature=0.3,
            response_format={"type": "json_object"},
        )

        return json.loads(content)
    except Exception as e:
        print(f"Error generating diagnosis: {e}")
        return {
            "possible_conditions": ["ç”Ÿæˆå¤±è´¥"],
            "department": "æœªçŸ¥",
            "urgency": "æœªçŸ¥",
            "advice": "ç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åå†è¯•æˆ–ç›´æ¥å’¨è¯¢åŒ»ç”Ÿã€‚",
            "reasoning": str(e)
        }
